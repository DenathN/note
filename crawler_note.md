# 如何实现一个爬虫


## robot.txt

robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被（漫游器）获取的

## 参考

1. [XPath wiki](http://zh.wikipedia.org/zh-cn/XPath)
2. [实现XML解析的几种技术(SAX、Pull、Dom三种方式)](http://www.lezhu99.com/detail.asp?id=1513)
3. [开源python网络爬虫框架Scrapy](http://www.cnblogs.com/derekDoMo/archive/2012/12/23/2829811.html)
4. [一个分布式定向抓取集群的简单实现](https://github.com/agathewiky/spider-roach?spm=0.0.0.0.OgQElC)
5. [xpath在HTML解析中的应用(加强版)](http://legacy.go4pro.org/?p=118)
6. [快速构建实时抓取集群](http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.KbbdPe)
7. [定向抓取漫谈](http://www.searchtb.com/2011/01/an-introduction-to-crawler.html)
8. [BloomFilter——大规模数据处理利器](http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html)
9. [Bloom Filter概念和原理](http://blog.csdn.net/jiaomeng/article/details/1495500)
10. [XPath 教程](http://www.w3school.com.cn/xpath/)
11. [Larbin 设计原理](http://itindex.net/detail/36916-larbin-%E8%AE%BE%E8%AE%A1-%E5%8E%9F%E7%90%86)
12. [robots.txt wiki](https://zh.wikipedia.org/wiki/Robots.txt)
13. [web spider wiki en](http://en.wikipedia.org/wiki/Web_crawler)
14. [oschina使用scrapy分布式爬虫的一个方案](http://www.oschina.net/question/209440_102205)
15. [scrapy实战之定向抓取某网店商品资料](http://my.oschina.net/taisha/blog/121608)
16. [gcrawler:一个基于gevent的简单爬虫框架](http://m.blog.csdn.net/blog/Raptor/6227219)